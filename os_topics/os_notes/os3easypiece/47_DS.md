### **Detailed Summary of Distributed Systems**

#### **Introduction to Distributed Systems**
- **Definition**: A distributed system is a collection of independent machines that cooperate to provide a service, often appearing as a single system to users.
- **Example**: Modern web services like Google and Facebook consist of thousands of interconnected machines working together.
- **Central Focus**: Building systems that work despite the failure of components.

---

#### **Key Challenges in Distributed Systems**
1. **Failure Handling**:
   - Components such as machines, disks, networks, and software can fail unpredictably.
   - Aim: Create systems that appear reliable to users even when parts fail.

2. **Communication Challenges**:
   - Communication is inherently unreliable due to:
     - Bit corruption.
     - Network issues (down links, severed cables, etc.).
     - Lack of buffer space in routers or endpoints.
   - Solution: Techniques to handle packet loss, corruption, and latency.

3. **Performance**:
   - Efficient communication is critical:
     - Minimize the number of messages.
     - Optimize for low latency and high bandwidth.

4. **Security**:
   - Ensuring the identity of remote parties.
   - Preventing unauthorized monitoring or tampering of communications.

---

#### **Opportunities in Distributed Systems**
- **Resilience**: By combining multiple machines, systems can achieve high reliability despite individual failures.
- **Fundamental Principle**: A system can rarely fail even if its components fail regularly.

---

#### **Communication in Distributed Systems**
1. **Basics of Communication**:
   - **Unreliable Nature**: Networking involves packet loss due to corruption, damaged components, or insufficient resources.
   - **Packet Loss**:
     - Causes: Transmission errors, lack of memory in routers, or overwhelmed machines.
     - Fundamental Problem: Loss cannot be entirely avoided.

2. **Dealing with Packet Loss**:
   - Use unreliable messaging layers (e.g., UDP).
   - Applications must handle packet loss explicitly when necessary.

3. **Unreliable Communication Layers**:
   - Example: **UDP/IP Stack**.
     - Fast but unreliable.
     - No guarantee of message delivery.
     - Includes checksums for detecting packet corruption.

4. **Checksums for Data Integrity**:
   - Compute a checksum before sending data.
   - Receiver verifies the checksum to ensure data integrity.
   - **Trade-offs**:
     - Strong checksums provide better error detection but are more computationally expensive.

---

#### **Code Examples**
1. **UDP Client-Server Communication**:
   - **Client**:
     - Opens a UDP socket.
     - Sends a message to the server.
     - Waits for a reply from the server.
   - **Server**:
     - Listens on a UDP port.
     - Responds to incoming messages.

2. **UDP Functions**:
   - `UDP_Open`: Opens a UDP socket.
   - `UDP_FillSockAddr`: Fills in the address details for communication.
   - `UDP_Write`: Sends a message to a specified address.
   - `UDP_Read`: Reads an incoming message.

---

#### **Building Reliable Systems on Unreliable Networks**
- **Key Insight**: Reliability can be achieved on top of unreliable networks by:
  - Handling packet loss, retransmission, and corruption explicitly.
  - Designing higher-level protocols like TCP for reliable communication.

---

#### **Important Tips**
1. **Communication is Unreliable**:
   - Design systems assuming messages can be lost or corrupted.
2. **Use Checksums**:
   - Ensure data integrity by verifying checksums.
3. **Performance Matters**:
   - Optimize communication to minimize latency and resource usage.

---

#### **Final Thoughts**
- Distributed systems underpin nearly every modern web service.
- They demonstrate resilience by leveraging redundancy and smart design to overcome the inherent unreliability of their components and networks.
- A solid understanding of communication, reliability, performance, and security is key to mastering distributed systems.

-------------------------------------------below is the code example which may or may not be necessary--------------------------------------------


Here’s a detailed summary of the provided content, including the explanation of the code examples:

---

### **Introduction to Distributed Systems**
Distributed systems have revolutionized modern computing. They form the backbone of services like Google and Facebook, which rely on thousands of machines working together to deliver seamless user experiences.

#### **Key Challenges**
- **Failure**: Machines, disks, networks, and software are prone to failure. The challenge lies in building systems that appear reliable despite underlying failures.
- **Communication**: Communication is inherently unreliable due to packet loss, corruption, or resource limitations.
- **Performance**: Optimizing latency and bandwidth is critical in distributed systems.
- **Security**: Ensuring identity verification, confidentiality, and integrity in communications is vital.

#### **Central Goal**
To build systems that work reliably even when individual components fail. The ability to mask failures while maintaining overall system reliability is the core strength of distributed systems.

---

### **Communication Basics**
Communication in distributed systems involves networking, which is fundamentally unreliable. Packets can be lost, corrupted, or delayed due to various reasons:
1. **Transmission Errors**: Bit flipping due to electrical or physical issues.
2. **Hardware Failures**: Broken links, damaged routers, or malfunctioning endpoints.
3. **Buffer Overflow**: Routers or endpoints may drop packets if their memory buffers are full.

---

### **Unreliable Communication Layers**
#### **UDP (User Datagram Protocol)**
UDP is a basic, unreliable messaging layer that doesn’t guarantee packet delivery or order. It is lightweight and ideal for applications that handle packet loss independently.

#### **Example: UDP Communication**

##### **Client Code** (Figure 48.1)
The client sends a message to the server and waits for a reply.

```c
int main(int argc, char *argv[]) {
    int sd = UDP_Open(20000); // Open a UDP socket
    struct sockaddr_in addrSnd, addrRcv;
    int rc = UDP_FillSockAddr(&addrSnd, "cs.wisc.edu", 10000); // Fill server address
    char message[BUFFER_SIZE];
    sprintf(message, "hello world"); // Prepare message
    rc = UDP_Write(sd, &addrSnd, message, BUFFER_SIZE); // Send message
    if (rc > 0)
        int rc = UDP_Read(sd, &addrRcv, message, BUFFER_SIZE); // Wait for reply
    return 0;
}
```

##### **Server Code** (Figure 48.1)
The server listens for messages, processes them, and sends replies.

```c
int main(int argc, char *argv[]) {
    int sd = UDP_Open(10000); // Open a UDP socket
    assert(sd > -1);
    while (1) {
        struct sockaddr_in addr;
        char message[BUFFER_SIZE];
        int rc = UDP_Read(sd, &addr, message, BUFFER_SIZE); // Read incoming message
        if (rc > 0) {
            char reply[BUFFER_SIZE];
            sprintf(reply, "goodbye world"); // Prepare reply
            rc = UDP_Write(sd, &addr, reply, BUFFER_SIZE); // Send reply
        }
    }
    return 0;
}
```

**Explanation**:
- **UDP_Open**: Opens a UDP socket on a specified port.
- **UDP_FillSockAddr**: Fills a socket address structure with the destination address and port.
- **UDP_Write**: Sends a message to the specified address.
- **UDP_Read**: Receives a message from a socket.

These functions abstract the underlying networking code, making it easier to send and receive datagrams.

---

### **A Simple UDP Library** (Figure 48.2)
The UDP library provides utility functions for handling sockets and sending/receiving datagrams.

#### **Code Breakdown**
- **UDP_Open**:
  - Creates a UDP socket using the `socket` function.
  - Binds the socket to a port.
- **UDP_FillSockAddr**:
  - Sets up a socket address structure for the given hostname and port.
  - Uses `gethostbyname` to resolve the hostname to an IP address.
- **UDP_Write**:
  - Sends data to the specified socket address using `sendto`.
- **UDP_Read**:
  - Reads data from a socket using `recvfrom`.

#### **Code Example**
```c
int UDP_Open(int port) {
    int sd;
    if ((sd = socket(AF_INET, SOCK_DGRAM, 0)) == -1)
        return -1; // Return error if socket creation fails
    struct sockaddr_in myaddr;
    bzero(&myaddr, sizeof(myaddr)); // Clear memory
    myaddr.sin_family = AF_INET;
    myaddr.sin_port = htons(port); // Set port
    myaddr.sin_addr.s_addr = INADDR_ANY; // Bind to any local address
    if (bind(sd, (struct sockaddr *) &myaddr, sizeof(myaddr)) == -1) {
        close(sd); // Close socket on failure
        return -1;
    }
    return sd; // Return socket descriptor
}
```

---

### **Reliability Mechanisms**
1. **Checksums for Integrity**:
   - Detect data corruption during transmission.
   - Common methods include CRCs and Fletcher checksums.
   - Trade-off between performance and effectiveness.

2. **Higher-Level Protocols**:
   - Reliable protocols like TCP build on UDP to handle packet loss, order, and retransmission.

---

### **Summary**
- Distributed systems rely on unreliable components but achieve high reliability through redundancy and clever design.
- Communication is the backbone of distributed systems, and managing its challenges is a central focus.
- UDP provides a foundation for building distributed systems, and libraries like the one demonstrated simplify its use.
- Techniques like checksums and reliable protocols are essential for ensuring data integrity and reliability.

This summary covers all critical topics, including the detailed code explanations, to prepare you for your exams.


----------------------------example of code ends here---------------------------------------------------------------------




### Detailed Summary of Reliable Communication Layers and Communication Abstractions

---

#### **Reliable Communication Layers**

1. **Introduction to Reliable Communication**:
   - Reliable communication is essential in distributed systems to handle packet loss over unreliable connections.
   - The sender needs confirmation that the receiver has successfully received the message.

2. **Acknowledgment (ACK)**:
   - **Mechanism**: Sender sends a message, and the receiver acknowledges its receipt by sending an acknowledgment (ACK).
   - **Process**:
     - Sender sends a message and waits for the ACK.
     - Upon receiving the ACK, the sender is assured the message was delivered successfully (Figure 48.3).

3. **Timeout and Retry**:
   - **Problem**: What if the sender does not receive an ACK?
   - **Solution**: 
     - The sender sets a timeout when sending the message.
     - If the timeout expires without receiving an ACK, the sender assumes the message was lost and retransmits it.
     - The sender keeps a copy of the message for retries (Figure 48.4).
   - **Drawbacks**: While this mechanism ensures delivery, it does not prevent duplicate messages.

4. **Handling Duplicate Messages**:
   - **Problem**: Duplicate messages may occur if an ACK is lost and the sender retransmits the message (Figure 48.5).
   - **Solution**: 
     - Use a **unique identifier** or **sequence counter**:
       - Each message is tagged with a unique sequence number.
       - The receiver tracks received sequence numbers to detect duplicates.
     - **Receiver Behavior**:
       - If the message ID matches the expected sequence number:
         - ACK the message.
         - Pass it to the application.
       - If it is a duplicate:
         - ACK it without passing it to the application.

5. **Optimizing Timeout Values**:
   - **Challenges**:
     - If the timeout is too small, unnecessary retransmissions waste resources.
     - If the timeout is too large, performance suffers due to long waits.
   - **Adaptive Techniques**:
     - **Exponential Backoff**: After each timeout, increase the timeout value (e.g., double it) to avoid resource overload during high packet loss scenarios.

6. **Real-World Implementation**:
   - **TCP/IP**:
     - Implements reliable communication with additional features like congestion handling and multiple outstanding requests.
     - Recommended for learning detailed network protocols and reliability mechanisms.

---

#### **Communication Abstractions**

1. **Purpose**:
   - To provide a suitable abstraction for communication in distributed systems.
   - Abstractions simplify system design and hide complexities of underlying mechanisms.

2. **Distributed Shared Memory (DSM)**:
   - **Concept**: Extends OS abstractions to distributed environments.
   - **Functionality**:
     - Enables processes on different machines to share a virtual address space.
     - Simulates a multi-threaded application where threads run on different machines.

3. **Working of DSM**:
   - Utilizes the virtual memory system of the OS.
   - **Two Scenarios**:
     - **Page is local**: Fetches data quickly.
     - **Page is remote**: Causes a page fault, triggering a fetch from another machine.
   - **Process**:
     - Remote machine sends the page to the requesting machine.
     - The page is installed in the page table, and execution resumes.

4. **Challenges with DSM**:
   - **Handling Failures**:
     - If a machine fails, pages stored on it become inaccessible.
     - Distributed data structures may become unusable (e.g., linked lists with missing "next" pointers).
   - **Performance Issues**:
     - Local memory access is fast, but remote access incurs significant delays.
     - Programmers must minimize communication between machines, negating much of DSM's value.
   - **Practical Usage**:
     - DSM systems are not widely used today due to these challenges.

5. **Lessons from DSM**:
   - Reliable distributed systems avoid using DSM due to its complexity and inefficiency.
   - Focus has shifted to other abstractions better suited for distributed environments.

---

#### **Key Takeaways**

1. Reliable communication requires mechanisms to ensure message delivery, detect packet loss, and handle duplicate transmissions.
2. Timeout/retry and sequence counters are essential for building reliable message-passing systems.
3. Optimizing timeout values and implementing adaptive techniques like exponential backoff can improve system efficiency and reliability.
4. TCP/IP is the most widely used reliable communication layer, offering advanced features beyond basic acknowledgment mechanisms.
5. DSM provides insights into extending OS abstractions to distributed systems but is impractical for building reliable systems due to challenges in failure handling and performance.




### Detailed Summary of Remote Procedure Call (RPC)

**Overview:**
Remote Procedure Call (RPC) is a programming language abstraction designed to simplify the execution of code on a remote machine, making it as straightforward as calling a local function. It hides the complexities of distributed systems, allowing the client to make calls and receive results seamlessly. RPC systems consist of two main components: a **stub generator** (or protocol compiler) and a **run-time library**.

---

### Components of RPC

#### **1. Stub Generator:**
The stub generator simplifies and automates the packing and unpacking of function arguments and results into messages, reducing errors and improving performance.

- **Input:**  
  The set of calls a server wishes to export to clients (interface definition). Example:  
  ```text
  interface {
      int func1(int arg1);
      int func2(int arg1, int arg2);
  };
  ```

- **Outputs Generated by Stub Generator:**  
  - **Client Stub:** Contains functions specified in the interface.  
  - **Server Stub:** Facilitates receiving and unpacking client requests.

---

#### **2. Client Stub Workflow:**
When a client calls a function (e.g., `func1(x)`), the client stub performs the following steps:
1. **Create a Message Buffer:** A contiguous array of bytes to store information.
2. **Pack the Message:** Includes the function identifier and its arguments.  
   - This process is called **marshaling** or **serialization**.
3. **Send the Message:** Communicate with the remote server via the RPC runtime library.
4. **Wait for the Reply:** Functions are synchronous, so the client waits for a response.
5. **Unpack the Results:** Deserialization (unmarshaling) of return values into a usable format.
6. **Return the Results:** Results are passed back to the client program.

---

#### **3. Server Stub Workflow:**
On the server side, the stub takes the following steps:
1. **Unpack the Message:** Extract function identifier and arguments from the incoming message (unmarshaling/deserialization).
2. **Call the Function:** Execute the actual function specified by the client.
3. **Package the Results:** Return values are marshaled into a reply buffer.
4. **Send the Reply:** Results are sent back to the client.

---

### Important Design Considerations in RPC

#### **1. Complex Arguments:**
Handling complex data structures (e.g., pointers or buffers) requires careful planning:
- **Well-Known Types:** Predefined formats for commonly used data structures.
- **Annotations:** Additional information to guide serialization/deserialization.

#### **2. Concurrency in Servers:**
Efficient servers use concurrency to handle multiple requests simultaneously:
- **Simple Server Model:** Handles one request at a time in a loop (inefficient if a call blocks).  
- **Thread Pool Model:**  
  - A fixed number of worker threads are initialized during server startup.  
  - Incoming requests are dispatched to available threads for execution.  
  - A main thread continues to receive and queue other requests.

- **Benefits of Concurrency:** Increases server utilization by allowing simultaneous execution of tasks.
- **Challenges:** Introduces complexity with the need for synchronization mechanisms like locks to avoid race conditions.

---

### Summary of Key Terms:
- **Marshaling (Serialization):** Converting arguments and return values into a format suitable for transmission.  
- **Unmarshaling (Deserialization):** Reconstructing the original arguments and return values from the transmitted format.  
- **Stub:** Auto-generated code that facilitates the abstraction of remote communication for both client and server.  
- **Thread Pool:** A concurrency model for handling multiple requests efficiently within a server.

---

### Advantages of RPC:
- Simplifies distributed computing by abstracting network communication.
- Reduces errors through automated code generation.
- Enables optimizations by stub compilers for better performance.

---

This summary covers all key aspects of RPC, including its components, workflows, and design considerations. It should provide a solid foundation for your studies. Good luck with your exams!

Here’s a detailed summary of the discussed topics, covering all essential aspects for your exam preparation:

---

### **Run-Time Library in RPC Systems**
The run-time library forms the core of an RPC system, addressing most performance and reliability concerns. It must handle several key challenges:

#### **1. Locating Remote Services (Naming)**
- **Challenge:** How to find the service the client needs to communicate with.
- **Approach:** 
  - Use existing naming systems like **hostnames** and **port numbers**.
  - The client must know:
    - Hostname or IP address of the server.
    - Port number used by the RPC service.
  - Internet protocols route packets from the client to the correct server.
- For advanced understanding, refer to **DNS and name resolution** or Saltzer and Kaashoek’s work on naming systems.

---

#### **2. Transport Protocol for RPC**
- **Options:** 
  - **TCP/IP (Reliable Protocol):** Ensures message delivery through acknowledgment and timeout/retry mechanisms.
  - **UDP/IP (Unreliable Protocol):** Requires the RPC layer to handle reliability.
- **Considerations:**
  - **TCP/IP:**
    - Simplifies reliability.
    - Inefficient due to extra acknowledgment messages:
      - Client sends a request → Server sends acknowledgment.
      - Server sends a reply → Client sends acknowledgment.
  - **UDP/IP:**
    - More efficient but shifts reliability handling (e.g., timeout/retry, sequence numbering) to the RPC layer.
    - Guarantees:
      - **No failures:** RPC executes exactly once.
      - **Failures:** RPC executes at most once.

---

#### **3. Handling Long-Running Calls**
- Long-running remote calls may appear as failures due to timeouts, leading to retries.
- **Solution:**
  - Explicit acknowledgments to inform the client that the server received the request.
  - Periodic updates from the server to reassure the client that processing is ongoing.

---

#### **4. Handling Large Arguments**
- Arguments larger than a single packet require fragmentation and reassembly.
- **Implementation:**
  - Network protocols often handle fragmentation/reassembly.
  - If unavailable, the RPC run-time implements this functionality (refer to Birrell and Nelson’s work).

---

#### **5. Byte Ordering (Endianness)**
- Different machines may use:
  - **Big-endian:** Most significant byte stored first (e.g., Arabic numerals).
  - **Little-endian:** Least significant byte stored first.
- **Solution:**
  - Define a common endianness in the RPC message format.
  - Example: **XDR (eXternal Data Representation)** in Sun’s RPC converts data as needed to ensure compatibility.

---

#### **6. Asynchronous Communication**
- **Synchronous RPC:** Client waits for the procedure call to return before proceeding.
- **Asynchronous RPC:** 
  - Client sends the request and continues with other tasks.
  - The client later retrieves results via a callback into the RPC layer.

---

### **The End-to-End Argument**
- **Principle:** Reliability must be ensured at the highest layer of a system (end-to-end) rather than relying solely on lower layers.
- **Example: Reliable File Transfer**
  - Even with reliable communication (e.g., acknowledgments, retries):
    - Data corruption can occur before sending or during disk writing.
  - **End-to-End Check:**
    - Compute a checksum of the file at both sender and receiver.
    - Compare checksums to ensure file integrity.
- **Corollary:**
  - Lower-layer functionalities can still enhance performance.
  - Evaluate the utility of these optimizations in the overall system design.

---

### **Key Challenges Addressed by the RPC Run-Time**
1. **Locating remote services:** Naming systems (e.g., DNS).
2. **Choosing transport protocols:** Balance between TCP and UDP for reliability and efficiency.
3. **Timeouts and retries:** Manage long-running calls and potential retries.
4. **Fragmentation and reassembly:** Handle large arguments across packets.
5. **Handling endianness:** Ensure compatibility across different systems.
6. **Enabling asynchronous calls:** Improve client-side efficiency during RPC execution.

---

This summary consolidates the critical details you need to focus on for understanding RPC systems and preparing for your exam. Let me know if you'd like any part clarified or further detailed!


48.6
Summary
We have seen the introduction of a new topic, distributed systems, and
its major issue: how to handle failure which is now a commonplace event.
As they say inside of Google, when you have just your desktop machine,
failure is rare; when you’re in a data center with thousands of machines,
failure is happening all the time. The key to any distributed system is
how you deal with that failure.
We have also seen that communication forms the heart of any dis-
tributed system. A common abstraction of that communication is found
in remote procedure call (RPC), which enables clients to make remote
calls on servers; the RPC package handles all of the gory details, includ-
ing timeout/retry and acknowledgment, in order to deliver a service that
closely mirrors a local procedure call.
The best way to really understand an RPC package is of course to use
one yourself. Sun’s RPC system, using the stub compiler rpcgen, is an
older one; Google’s gRPC and Apache Thrift are modern takes on the
same. Try one out, and see what all the fuss is about.