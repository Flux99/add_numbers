00:00:00	hey everyone welcome back to the channel today we're going to be building out a distributed job scheduler now about 3 days ago I put a lot of thought into this video but since then I decided to procrastinate and record on Saturday morning meaning that I went 0 of3 at the bar last night and I'm now brutally hung over uh so I'm going to free ball this thing and we're just going to see how it goes all right I've procrastinated enough let's do this so what is a job scheduler at its core well really all

00:00:25	it's doing is basically you taking some piece of binary or you know some EXT to execute on some sort of computer putting them up somewhere and then basically allowing us to run them on some other node so for example let's imagine that we actually have one that is a dag or a directed a cyclic graph what that might look like is this lovely oblong structure right here where uh you have a 3 P.M Cron job another 5pm Cron job and then once those both run successfully you can actually run this node over here

00:00:56	because it actually depends on the execution of the both of them so the whole point of the dag is that it is a cyclic meaning you know uh once one node finishes and another node finishes eventually we can complete the whole thing you're not running in an infinite Loop there is actually like a completion process here cool so let's talk about some quick problem requirements to formalize this thing out uh the first is that basically we want to be able to run uploaded binaries uh upon a user request

00:01:22	right so the binaries will probably go in an object store like S3 I'm now realizing that I didn't draw this in my final diagram but I'm going to say it explicitly now so that everyone remembers the binaries go in S3 you access them from S3 on some sort of exeutive and then you run them there the next thing is that ideally every job just runs once in practice we're going to find that this is very very hard to do without a series of two-phase commits which would make this whole thing very slow and perhaps infeasible so we're

00:01:49	going to talk about ways that we can ideally mitigate the effects of running jobs perhaps more than once while you know basically acting as if they had only run once okay number three is that user should be able to see the status of their jobs um if I'm running a a binary or I want to you know run it on some sort of KRON schedule I need to be able to know uh whether it failed whether it succeeded what the error messages were things like that and then so besides just running things uh on a cron we also

00:02:15	want to be able to run dags so that's like I mentioned a directed a cyclic graph of task schedules so you know we've got one dependency here another dependency here and then that's going to lead us to be able to run some other task and uh I'm not going to go too much into capacity estimates because at the end of the day if you're making a job scheduler it's going to be different for every company that you're doing it for um but we are going to be running millions of jobs a day and so as a result of that this thing needs to be

00:02:39	fast because if it's not fast and it's not highly available uh many developers in our company are going to have problems with their jobs and then uh we're going to be in a lot of trouble cool so let's first do a high level overview uh because someone in my comment section told me that uh everything I say is incomprehensible unless I present this way so we're going to go ahead and do that so we've got some sort of of recurring task scheduler right so that's going to be useful for our Crown jobs which are basically

00:03:04	saying you know run once a week run once a week at 3 p.m. on a Monday uh our dags which are the graphs and then that is basically going to take tasks and put them in our scheduler node so we've got some sort of schedular table which is getting pulled let's say once a minute or once every 30 seconds or something like that and then all of the tasks in that 30second batch that we decide hey we actually want to run these we're going to go ahead and send them to an executive which is basically going to

00:03:29	read from three that binary and then put the job somewhere in some sort of status table which we'll call our job history our status table that eventually if I'm a user I can go ahead and read from cool okay so the first couple of t uh things that I'm going to talk about in this video are actually going to be task scheduling right so that's going to be the process of well if I've outlined that I want a certain task to run on every interval or within a part of a directed asly craft how am I actually

00:03:55	going to get them from there into the scheduler node cool so oops scroll a little bit too far the first thing that I will talk about is the actual cron task scheduling right so the idea here is that let's say I want to run this job every Monday at noon we can have two tables right we've got one that actually just outlines the settings of the Cron job so the metadata of it when we actually want it to run maybe the ID of the overarching parent job and then we've got a second one that actually uh includes the scheduled tasks

00:04:23	that's a result of it so when I first create a task my proposal here is to basically at first we're going to put one event in the cron jobs table saying we're going to run this thing every Monday at noon and then we're also going to put another event in the scheduled tasks table saying hey this upcoming Monday at noon we have to run this task right so just just the the soonest uh next upcoming Monday at noon we're going to put in the task table and we're both going to do this on first upload now

00:04:49	note here that um I'm kind of making this look like I'm using two-phase commit eventually in my final diagram I will reveal that I'm going to uh bait and switch you guys and use change data capture to do this type of thing where you know we'll basically have this guy doing some change data which goes into this guy but the gist is eventually what's going to happen is our executive is first going to pull from our actual scheduled task table right this is step one right here it's going to see that

00:05:16	we're running this thing at noon today and so step two is that it's going to now uh go over to the cron jobs table and then after it's run the job it's going to basically say okay this current version or this current iteration of the job has been successfully completed completed now go back and put uh the next iteration of that Crown job in our scheduled task table so that would be noon for next Monday as opposed to this Monday so yeah basically the idea here is that the executive is going to schedule the next Crown instance

00:05:43	shouldn't be too hard the thing that's a little bit more complex is actually going to be scheduling dag jobs so the way that I'm going to propose doing this after doing a little bit of research is basically that we want to have some dedicated database table devoted to Dax now I suppose you could do all of this in the scheduling table uh but I don't think that would make our lives very easy and I think it would lead to a lot of clutter in that table which we want to be relatively fast because we're

00:06:08	going to be polling it all the time so let's imagine we've got the following setup for a dag we've got jobs 1 2 3 4 and five one and two are going to be the root nodes over here and they're triggered at 3:00 and 5:00 p.m. uh respectively job three depends on the completion of 1 and two job four depends on the completion of job three and five also depends on the completion of job three so the way that I'm going to outline this in some sort of database table is as follows so I'm going to have my job

00:06:36	ID I'm going to have the cron schedule if it exists there should only be a Chron schedule for those kind of um you know root nodes at the top of our dag we should have uh some concept of who the children are the reason I say this is because you know when I finish job one I need to now know okay I'm going to job three and I'm updating that one of our dependencies is finished right so when I finish job one I now need to go to the ID3 which is over here and I need to say ooh you know what my dependency for job

00:07:04	one is finished so now instead of epoch uh zero for job one that I've seen I can actually cross that out and put in an Epoch one and so by doing this let's imagine now that job two finally finishes as well it's 5:00 p.m. and we schedule it we see oh we've got a child node of three so I've got to find job uh job ID 3 in the table and I can see the job two is the one that just finished maybe uh this particular Epoch was epoch one and now what I can add is some additional database logic you know within my transaction to basically say

00:07:36	okay well if all of the dependencies are now once again on the same Epoch I now have to schedule this job so because job three has all of its dependencies on Epoch one now I can say o it's time to run Epoch one for job three so when job three gets run I'm now going to see its children or job four and job five and I'm going to go over here respectfully and say ooh three was just run with Epoch 1 now I can see that all of the dependencies for uh job number four have been run I can schedule it same thing

00:08:04	applies for number five we just ran with Epoch 1 so now I can go ahead and schedule it so the one other trick that I'm adding here is that uh again we do need to eventually schedule jobs one and two again because these guys are on a Cron job schedule and the way that I said that worked with all the KRON stuff was that you basically have to have the executive reschedule again so what I've done is I've added like a little uh mini trick here where I'm basically considering um jobs one and two are root

00:08:31	tasks as actual children of jobs four and five and so by doing that you know we can actually say that jobs 1 and two have dependencies of jobs four and five so that way I will only schedule jobs 1 and two over again once four and five have run successfully so that way we're not you know running multiple iterations of the dag at the same exact time cool so hopefully that generally makes sense here okay if I'm going to actually outline this is a formalized process the main idea is that again all of these

00:09:03	things are going in the dag table uh the roots of the dag are going to first be scheduled when I first upload the dag to the table with Epoch one that's just going to go right in the scheduling table uh as far as figuring out what the actual roots of the dag are uh depending on how you accept um the input from the user you may need to do something known as a topological sort which is basically taking your dag and figuring out which nodes have dependencies on other nodes and then figuring out which can be run

00:09:28	right off the bat and which need to wait for other nodes to run cool and then as we complete those tasks basically what we want to do like I mentioned is just Mark that their Epoch is completed in all of their children nodes so that's going to be some sort of atomic transaction right there because we have to uh modify multiple rows at the same time and then finally when all of those dependency tasks have an equal Epoch for a given row right so if I'm a node three over here when I see that both node one and node 2 are on Epoch 1

00:09:56	that means I can run uh the uh job three for epoch one so I can go ahead and schedule that task and then finally once we complete all of those Leaf node tasks we can re-trigger Our Roots again but and the way we model that is by basically acting as if this guy is a child of both four and five and that this guy is a child of both four and five cool so there are going to be a couple of edge cases here right so let's say job three were to error out now all of a sudden I have to actually propagate through job three and still acts like

00:10:26	job four and job five were completed albeit with errors so so that one and two get rescheduled again for their uh for their Chron schedules cool so as far as the table choice for the dag table because we have started to uh speak about this a little bit um I do end up saying that we use my SQL here however I'm eventually going to go back on this which I'm now realizing because I made this video over the course of two days but the gist is we do need atomicity uh because we're updating multiple rows at

00:10:51	the same time so we are going to want to be able to use transactions uh and ideally we don't want to be doing distributed transactions right we want to Shard our graph or rather Shard our data such that one graph is completely encompassed on a single node otherwise we're going to have to be doing a bunch of two-phase commit to modify most of these rows at the same time like I mentioned as well in the case of an error we basically have to propagate down all of the children of that particular error node to mark them as

00:11:14	completed albeit with an error because they can't run and uh as a result of that that's another example where we're just going to have to update many many rows at the same time number two is that we want to do all of these updates on a single node like I mentioned that's just going to be how we Shard and so we want a table that uh supports transactions we're just going to sh it on our dag ID I say my SQL here but I think ultimately that I've chosen that we should go back on this for a couple of reasons uh I

00:11:38	think is going to be the better choice because it's going to enable us more data flexibility in terms of how we actually want to represent our dag uh is obviously going to be a document-based database where you can use Json and so you know in my SQL depending on how I want to represent these dependencies and these children over here I think I would lose a little bit of flexibility compared to Mango so that's ultimately what I end up using at the very end cool as far as the scheduler table goes uh this is kind of

00:12:06	going to be the main part of the job scheduler this is what I want to focus on the most so we've got our job IDs that's going to be our primary key we've got our S3 URL which is where the binary is going to be located in S3 right I have to basically put in a binary to S3 to say the executive can go download this and run it and then also we need some sort of run timestamp so the interesting thing with this run timestamp and I'm going to add one more column obligatory called status which we'll talk about a little bit later but

00:12:33	the main thing with this run timestamp is that basically it's going to tell me when to run my job so let's say it's 2011 for my run timestamp and now I happen to notice when I'm pulling the database that it's 202 I'm going to say oh shoot you know what it's time to run the job however there are a few things that are nuanced with this runtime stamp at every step of me handling the job I actually want to increase the runtime stamp by a little bit so let's say I finally get this job on an executive I

00:13:01	might say okay I'm going to increase my runtime stamp by 5 minutes because if for some reason that executive fails we actually want this runtime stamp to indicate a timeout time and if you know if we just keep 2011 the entire time we're going to keep retrying this job although it may be running successfully what we want to do is add a few minutes to it so that you know if our job times out for some reason we'll retry it but we aren't just retrying it at every single iteration so the main idea here

00:13:27	is we index everything by a run time stamp so that we have a relatively quick query on this database to quickly find all of the jobs that we need to run so you know let's say the current time is 206 I know that both of these guys should be run because the run time stamp is less than that that being said you know if maybe an executive handles this guy and it switches it to 215 and it's currently 206 I know I'm not actually incing this job right here cool and then again like I mentioned every step of

00:13:53	actually running our job we want to update that run time stamp to reflect how much time we should wait before timing out and basically retrying the job in its entirety cool so another thing that I want to quickly talk about is actually ensuring the performance of our scheduling because at the end of the day we're reading this table a lot there are going to be a lot of things in this table and we want to be sure that it's relatively fast so there are a few things that I've thought of doing uh in order to make

00:14:20	sure that all of these reads are going to be as fast as possible uh the first is going to be that we could keep our data in memory that might not be the best idea just because there's going to be a lot of tasks to schedule like I mentioned billions and so as a result it could get a little bit expensive to do that um but additionally keep in mind that besides actually just reading all of these tasks we're actually constantly rewriting these tasks too and as a result we're reordering our index because of the fact that um you know

00:14:47	we're rewriting all these timestamps and we're basically reordering a sorted list all the time which can get a little bit of uh expensive right because these are all going to be uh basically rights where we have to resort our list based on the updated time stamp that we just put on this job and then we're also simultaneously reading from this table and we're reading the whole uh the whole thing more or less to to get a sense of which jobs we want to run and so in order to read that table we have to grab

00:15:11	a bunch of locks and in order to write we have to grab a lock on that row to write so we're going to have a lot of contention there so we want to be pretty smart about this so I think realistically the the two most likely options here are going to be one we can have many different partitions of the schedular table because why not there's no reason that we can't just have you know 10 different scheduling databases and having them all have something pulling them and then incing things to executiv as we need and another possible

00:15:37	option is that if we really want to avoid grabbing any locks uh to read from the node that we're sending all these rights to we can actually read from a read only replica of the scheduling node that being said in my opinion uh this can get a little dodgy just because you know what if our leader goes down and then we don't see an update in the replica because we're not using strong consistency or something like that that being said um we do have a retry time stamp so the main consequence of this

00:16:03	would actually be that we probably end up just retrying more jobs and uh yeah it's it's not the worst idea either I think this is something that's feasible too cool as far as load balancing goes this is another key aspect of the problem so the question is we've got all of these executors right here all three of these random nodes we can run the task on pretty much any of them how do we ensure that we're getting the most usage out of our executors as possible and that you know one of them's not just

00:16:30	sitting idle so option one and this may be a little bit naive would be to use something like a load balancer so in my opinion this isn't going to be too ideal so the reason for this is let's say we did some sort of consistent hashing so that based on a job ID we assign a task to a specific executive node well it is possible that one of those nodes could just get a super long running job and then all of a sudden now we're trying to push more jobs to it but we can't you know we have to wait for that first job

00:16:56	to finish so you might now say to yourself okay well consistent hashing obviously isn't good here the load balancer should have a sense of basically you know whether every single executive is running or whether it's not and in my opinion that's also a little complicated because now we've got every single executive having to send a bunch of pings to just one place to basically update its status and that is going to be not ideal because that's a lot of load for one node to handle the node being the load balancer so it kind of

00:17:25	becomes a point of failure option two is to use some sort of message broker and this is a little bit easier because now all of a sudden the messages get routed right to that executive so out of our message Brokers we have two different implementations of them and I'm going to go through them quickly the first is going to be a log based message broker like Kafka and so typically what we would do there is we would do one consumer per partition however even this is not going to be great because let's say you know we've

00:17:49	got a B and C in our Kofa q a takes super long to run now all of a sudden our executive is stuck running A and B and C are stuck in this partition right they've already been put here and no one else can read them because we have one consumer per that partition we might have a second executive sitting here it could have gone through its entire cofa que and it can't run uh it can't read BNC so actually in the case of something like a job scheduler what we wouldd be better off doing is using something like

00:18:15	an inmemory message broker now you may be thinking to yourself This actually looks like a very similar design pattern to what we did on YouTube right where we do all this pre-processing of the videos encode them in a bunch of different formats and uh in order to do that because we don't really care what are doing the encoding it's kind of the same idea we're using this inmemory message broker to actually do our load balancing for us so basically the idea here is we have got a bunch of different consumers

00:18:38	reading from this message broker and basically when they're idle that is when they're going to take a task off of the queue so this is going to make our life a lot easier ideally as well you know because it's in memory uh it is going to be relatively low latency um and just quickly return tasks over to those executiv to be dced cool so another thing to note is that at first I've been acting like all executives are equal and the reality of the world is that they are not uh certain times you might have more

00:19:06	expensive Hardware on certain executors others might have more inexpensive hardware and it is possible that in some cases we might want to Route certain jobs to certain executors so in this case we could actually use multiple levels of these cues right so all of these would be different active mq instances or rabid mq or some other JMS implementation and so the idea here is that we're actually going to be scheduling tasks just like an OP operating system does so in operating systems you have something called the

00:19:32	multi-level priority cue where basically you try and run a task on some sort of resource uh and if you're unable to do so within a certain time limit you actually basically just you know say screw it I don't really care what this is going to return me and then recue it onto a higher priority level queue which is you know it's going to be more likely to be run and in our case we could actually have the more powerful executors reading from those higher priority level cues so let's say you know our lowest level executive is going

00:19:58	to have a 10-second time out so that if we have some sort of really expensive job that can't finish in those 10 seconds we say you know what screw this we're just going to retry it and then we're going to retry it on level two with a one minute timeout still can't finish screw that we're going to retry it on level three with a 1 hour timeout and so level three you know these executors over here maybe have more expensive hardware and then additionally note that those guys should also be reading from

00:20:24	the level two and the level one cues because at the end of the day if there's nothing in level three should be able to take on more work they shouldn't just be idle so ideally they should prioritize running the jobs from the level 3 q but uh if there's stuff available in level two and level one they can go ahead and do that too so the point of the scheduler here is when we're actually retrying a job we can keep a count of how many times we've retried it and use that count to actually determine the

00:20:48	priority level of the of the message cue that we're going to send our job to cool so as far as job completion goes what do we want to do about this well if we have a scheduled job a user needs to be able to query its status right we need to know whether it completed or whether it failed or whether we need to retry it and uh the reason for this is so that we don't retry it further right if I never mark my job in the scheduling table as completed or failed or I never remove it from scheduling table uh we're

00:21:16	just going to retry it an infinite amount of times and that would be bad so uh what we can do like I mentioned we're going to add some sort of status column to our scheduling table uh and basically the gist is like I mentioned uh we're going to retry jobs based on their time stamp but also if a certain job um you know has a status of completed or failed it implies that we've run it as much as possible and that we shouldn't retry it and so basically the gist is here now as opposed to just indexing on our

00:21:43	timestamp of the job we should index both on the status and the time stamp of the job so that way uh you know we just basically want to ensure that we're only running jobs where the status is not equal to completed SL failed and the status is just equal to either like a null or an in progress or something like that and then the idea is that users should be able to query this table to see the actual job status now keep in mind that this table is also going to be the same as our table that's actually doing the

00:22:09	job scheduling and so we want to minimize the load on there so at least in my opinion I think we should probably have some replicas only or basically some readon replicas that the user should be able to read from as opposed to having to read from that main leader scheduling table because otherwise we can really slow down our performance doesn't really matter if a user you know it takes him an extra 30 seconds to see the status of their job they just want to be able to see it eventually and so eventual consistency should be fine here

00:22:35	uh as far as partitioning goes I think uh some combination of time range for this scheduling table as well as just some random number based on the job ID you know you can hash it uh would be good to distribute out the load across all of these scheduling tables cool so the last aspect of this video that I'm going to talk about is actually going to be how we run our jobs and ensure that we do so just once so the main point here is that uh we have all of this retry logic built in right from our scheduling node we've got uh

00:23:05	these time stamps and so as a result of that um not only are we prone to retrying jobs multiple times we don't need to but also it's possible to due to network partitions or network failures that we are just going to end up running things on multiple places so what would be example number one let's say this is an executive and the executor dqs from our active mq or Robin mq and then the acknowledgement going back to the que to say hey take this thing out of the queue never reaches it right it breaks down well the queue is now

00:23:36	going to deliver that over here as well to another executive now we're running it TW uh two executiv in the same time option two is we've got ourselves a scheduling node uh the scheduling node just you know uh the job is taking too long to run it retries it because the time stamp has run out or or rather the the retry time has now been reached um option three is active Q active mq just goes down and so again we reach ret try as a result of that option four is that one of the executives go down and we

00:24:03	retry as a result of that so again uh we have a lot of different places where because of our fault tolerance uh we are prone to running jobs more than once cool so what our retries are going to ensure is that everything runs at least once but what we want to ensure is that it runs only once well can we do that well the big thing that we actually want to avoid in my opinion here is going to be running the same job at at the same time on multiple places because then you can actually start overwriting state

00:24:33	right so let's say uh the job has some sort of side effect where you write to a database uh or you're reading from a database and writing to some rows as well if two things are running at the same time then they can start conflicting with one another and it might corrupt the state of the job so we definitely don't want that to happen how can we stop it the answer would be a distributed lock so we would have something like zookeeper over here which as we know is basically uh a mini database or a mini distributed log that

00:24:58	uh you use consensus between the nodes to achieve it so that way we can be sure that uh if we're reading from the same leader that we're achieving strong consistency if we're uh you know reading from The Zookeeper leader and so what we can do is our executiv can go ahead and just grab a lock for the job ID that they care about and then that way only one of them is going to be able to run it at the same time now it is unfortunately the case that zookeeper is not going to be perfect here either the

00:25:23	reason being that we probably do need a TTL on these locks because let's say I grab this lock for 22 and then my exeutive goes down I do want to be able to rerun this job right like I don't want to have that lock grabbed forever and then 22 is never going to get run but by virtue of having a TTL it does mean that if this thing is taking too long and it loses its lock now all of a sudden this guy can come up grab the lock and then if this guy comes back now they're both running jobs at the same time so again nothing is perfect in this

00:25:50	situation it is always going to be the case when dealing with distributed systems that you're prone to just some sort of ridiculous failures nothing can be super robust unless you're using something like strong consistency and then our performance would be really really bad now it is the case that you know it's not the end of the world if two jobs do happen to run at the same time right like we're we're not talking about human lives here uh but you know your developers is going to look at that and be like what the hell happened to my

00:26:13	job I can't tell why this thing failed they'll rerun it and then hopefully everything goes better that time so another question is well we can stop jobs from running at the same time can we make sure that we're only running jobs exactly once so like I mentioned jobs are getting triggered pretty re uh pretty frequently based on those timeouts and that kind of retry Tim stamp uh so the question is what can we do to actually stop that from running more than once in the erroneous situation where we actually retry a job

00:26:40	that we shouldn't be well option one is that we always run jobs on the same executive note that I write dumb right here because it is uh the whole point of the job scheduling service is that you don't care what Hardware it runs on or rather that you run on any hardware that's available so to have to basically limit yourself to running on one specific computer kind of defeats the purpose of the job scheduler you could just run it on your own computer at that point number two is that uh basically somehow all the nodes

00:27:07	would have to basically check our scheduling table when they get a job to see if it's been completed uh but note that that is basically prone to all sorts of race conditions that's not perfect by any means uh another node could just upload to the status table saying the job is finished you know right after a second node starts running it and then the third option is that we could make our jobs item potent or deal with the consequences of not making them item potent is it the end of the world if I send two emails to my user probably

00:27:35	not uh and you know for things that actually have important state we should probably make them item potent if we're syncing to some database uh it should probably be that our jobs are deterministic for the time that we're running them this is just generally uh you know good practice when running batch jobs so this is probably how I'm going to cope with the situation and just say you know make deterministic jobs or you know deal with the fact that your jobs are not deterministic cool the final thing that we're going to

00:28:01	go through is my diagram this is the one that I made this morning so if it is uh incomprehensible due to my hangover I apologize in the service like this there's basically two main things that we can do right the first is that we can inq jobs the second is that we can get the status of the jobs that we've inced so like I've mentioned we've got one table to handle all of our dag metadata another table to handle all of our Crown metadata and I mentioned that when we upload a dag nice voice crack Jordan

00:28:29	when we upload a dag or when we upload a cron what we want to do is also schedule the first instance of that dag run and the first instance of that cron run into our actual scheduling table over here now we could do this with a two-phase commit and that definitely wouldn't be the end of the world because the end of the day it's just one upload on the part of the user um but to over optimize things I've decided to just use change data capture the reason being that now we don't have to do a two-phase commit

00:28:53	and I do know that eventually this thing is going to flow through Kafka flow into basically our consumer of that change data and then into our scheduling table great so from the scheduling table once we have all of those tasks in queued we know that we're running them on some sort of interval by having some sort of schedule node polling the table running that query and then putting them in our active mq priority cues for executiv to actually run those jobs additionally we've also got some readon scheduling

00:29:21	replicas which we're going to be using for our job status uh for users right because I mentioned if the user is actually reading from our main table over here now we're adding a bunch of additional load and that's not necessarily a good thing we want that polling process to be as fast as possible so that we can enue as many jobs as possible in a short interval um another thing to note is that when executiv do finish tasks they have to update their actual status so they can do that over here in the status table

00:29:47	additionally for dags we basically need to figure out oh well besides actually updating the status of my current job does the triggering or rather does the completion of this job actually trigger other tasks in the dag so in order to figure that out you actually have to go all the way up here over to your dag table uh you would go ahead and update the epoch numbers accordingly as we've mentioned and then that again can then flow through our change data capture through Kafka through our dag task

00:30:12	queuing node and then back into the scheduling note so hopefully this mostly makes sense um I feel like I've explained it uh in the previous slides as well and uh yeah my head is killing me a little bit so I'm going to jump off but have yourselves a great week guys and I will see you in the next one

