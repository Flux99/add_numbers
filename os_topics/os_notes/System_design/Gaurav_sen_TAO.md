00:00:00	Hi everyone!. In this video we are talking about another white paper. It's Facebook's graph store. And the name for this is T.A.O which stands for the associations and Objects. As you can expect, associations means edges and objects means nodes in a graph. So a node may be connected to another node. And this would be an association. In Facebook's case this is a social graph. So this could be a person. But if he checks into a hotel that could be an association. So checking into a hotel is an action.

00:00:49	And then this would be in hotel. In general these associations map events and objects map repeatable actions. A good example of this is comments. If you add a new comment, the source will be the same, but the destination will be a separate comment. So in this way, you're able to map objects into nodes and you're able to map actions or any kind of relations to edges. Facebook has a very, very large graph, a very large social graph. It comes in petabytes of data. Another challenge for Facebook is

00:01:28	you have 1 billion queries. Per second on the social graph. This is a paper from 2013. So this is all pre pandemic pre everything. So I'm sure this number would have been x by now. Right. So we can assume that maybe 10 billion queries per second is possible. But the paper talks about technology which helps them reach 1 billion queries per second. One interesting fact about this is about 99.8% of these queries are read queries. If you have seen the white paper video that we took for memcached that also has

00:02:09	similar patterns, read queries on much, much more frequent than write queries. So some optimizations can be picked up from here. As you will see, the association objects does it. Now imagine you are in 2013 and you are working at Facebook as a principal engineer. A lot of the engineers are complaining about. I try to store the graph in my local memory, but I can't do it. I've started looking at databases which perform well. When it comes to graph queries. One of them is new for J. But the challenge here is that you need a billion queries per second.

00:02:44	Doing this on disk is going to be quite slow. So one thing that you can do is try to keep this data, this graph data in-memory. Okay. So instead of new for J you're looking for an in-memory graph database. It doesn't really exist at the scale that you are looking at. Petabytes of data. There's nothing that. So you have to build something. Now you can build things from scratch as a principal engineer. It's probably going to sound cooler, but if you're looking to drive impact, then you want to use the existing infra of a company,

00:03:17	if you can, to meet all requirements. And at this point in 2013, Facebook is a master of MySQL. They're very, very comfortable with this database. And they have also built an extremely efficient cache. Memcached. Memcached is an open source project, but Facebook has made significant contributions to it, and they are experts with this piece of technology. So if you are looking for a graph store or in-memory graph store, if you could write a wrapper on top of memcached. If you can use this as a store, then you might just pull this off.

00:03:49	That is the idea that the engineers at Facebook decided in 2013 we are going to use memcached. We are going to have it backed by MySQL, but we are going to write wrapper APIs on top of memcached, which will make the cache behave like a graph store. So let us just note this down. The first thing we want to do is we want low latency. And the way we are going to solve this problem is by keeping things in memory. We also want to graph store. The way we are going to do this is by having API wrappers

00:04:28	over memcached. It must be easy to use as a principal engineer. Always look to build technology which is easy to adopt. Easy to use, easy to understand. You do not have to know the internal workings of a system if you have to use it. Unless you do this, people are really not going to use your system. In fact, this is one of the motivations for creating the graph store. Facebook engineers would need to understand how memcached worked. Memcache has its own onces. It works in a particular way. The engineers would need to understand

00:05:00	how memcache works and then file graph queries on top of it. A lot of common optimizations that all of these engineers made were moved into a single graph store by the senior engineers at Facebook. That's the major motivation. Most technology, most engineering products are made for ease of use instead of high efficiency. Okay. Efficiency can be made, but ease of use really needs design principle. Okay, so this is a core requirement. You are also looking at a high hit rate okay. You want to observe access patterns in your system

00:05:40	and then have your technology mimic or solve for these access patterns. There is no perfect cache. There is only caches which are designed to solve a particular problem. Let's first pick this up. The API is you have to expose should make this field like a graph. They are creating, reading, updating or deleting and association. This means when Gaurav sends a friend request to promote Varma, I should be able to use an API which says and association. Gaurav. That is the source. Pramod. That is the destination.

00:06:33	And then the type of the association is friend request. Okay. This should be a simple API for me. I can also say get me all the friends of Gaurav. So the source is Gaurav. The type of association here is friends. And if I want all the friends of Gaurav, then that is fine. I just need to send this. But if I want the top 100 friends, I've got it. Usually in a web page. When you see your friends list, you want to see the top hundred most active friends on Facebook or the people you talk to the most.

00:07:07	So over there you can give a limit. You can also mention time ranges, meaning in the last 15 days get me my top 100 friends. So maybe the most recently added friends will come over here. That's now -15 days. This is the start time. And now is the end time. You can also paginate the requests. Just imagine you are watching your friend list the top 100. Then you press on next. So it is going to be a separate API call. You could also get the count of friends that Gaurav has. So if I want to know how many friends I have,

00:07:42	I don't want to go and fetch the entire friend list and then count. I just want to keep that count somewhere. Okay, so these metrics like how many friends do I have or how many places have I visited? They are useful not just for analytics, but also for displaying to users. In this case, you just need to send the source and the type. So the number of friends Gaurav has type will be friends sources. We got that. This can also give you the number of people who are checked in total. So you have a hotel in Las Vegas that is now the source,

00:08:12	and there is various people who have checked in. If you find this query of count, you're going to get the value of three. How is this exactly happening? I mean, how can you have source of Las Vegas hotel isn't it person to Las Vegas? So Facebook does a clever thing here. It has bidirectional relations. So every person can see how many hotels they have checked into. At the same time, the destination has a inverse relation. So instead of checking is going to be checked in. Apart from this, there are no other APIs that targets gives.

00:08:49	Okay. It's a very minimal API site. In fact, this can only give you paginated responses. Is it the best approach? You have to remember that there's a billion queries being made on this graph. Score a billion read queries. If you try to add a lot of functionality to this production intensive graph store, it's possible that its performance is going to deteriorate and eventually people will stop using it. Adoption rates will be low when adoption rates go down. Engineers spend time building their own pieces of technology.

00:09:20	You don't want to do that. You want to keep things highly performant, which is limited feature set or basically limited API set. Now let's see how the graph store at Facebook is actually made scalable and performant using its architecture. Okay, so first thing is that the graph store is backed by MySQL. We talked about this earlier. You can't have a single MySQL server managing the entire graph store. Like we said, bytes of data is a lot of data. And this is distributed across the world. So you need multiple MySQL nodes running all types to scale out.

00:10:04	And the second thing is you have multiple core caches running. So it's a distributed store. Again. When a person queries for an object, they are going to. Go to the server. The application server at Facebook. The app server is going to be running a client, a small piece of software. We spoke about this in the memcached video. The client is going to make the API call for this object, and the object is mapped to a shard, a shard. So you have multiple servers managing different key ranges. Let's say key one 2000 is what you have in total.

00:10:43	So one two, 333 is going to be stored here. All keys between 123 33 will either belong here or will result in a cache miss and will require a database call from 334 to 666. You have the keys here and from 667 to 1000. You have all keys mapped here, so it is not necessary that you are going to have 333 keys here. This is the key space. In reality, maybe your side is not that big. So you can only store 100 keys. So the 100 most popular keys in this range are going to be stored in each shard. Now you're looking for this object.

00:11:20	If you go and query this shard right, you have the mapping of each object to shard. If you do not find the key you go and query MySQL. So this would happen in a cache. Miss. Also, if you are writing, if you are writing a value so you want to add an edge. Gaurav is now friends with Remote Burma. Then you get a write operation sent to MySQL. This keeps the architecture simple. If things are not found or if things have to be updated, just go and hit the database. Everything else will work out by itself.

00:11:57	You are, after all, using memcached. So memcached has its own ways to take care of consistency. Now how does this client know where this object resides? The shard to object mapping. How does the client know about it? You hash the object ID. To get a value. Let's say one two, three. And this value then is searched in the ranges of the shards. So 123 lies in this range. That is reason why you are sending it here. If you hash this object, maybe you get a value of 400 that lies in this range. And when you hash this edge or do you have an edge?

00:12:37	Let's see. You get a value of 900. Okay. Hashing objects is pretty straightforward. You just have to hash the idea of the object. But hashing edges is a little tricky. Remember that your API say that I can get you all edges from a particular source. So why do you want to keep all the edges from a particular source? You want to keep all them together. If you want to find how many friends God has, then you want to keep all of the edges for God's friends in one place, along with the God of Node also.

00:13:12	Okay, this is because of the access pattern, the APIs that you're exposing. Usually you give a source. You give the type. With that, you can go and search everything for that source. So ideally you want to keep the hash of the source ID. Used for routing the edge object to a shard. Let us clearly understand this concept. If you have an edge from a source person one, two, three to a destination, post 345 where the type is like so. Person one two, three has like a post 345. Then which shard does this edge belong to?

00:14:01	Well, if you have three shards and they have the ranges zero two, 200, 202, 400 and greater than 400 goes here. Then the source says that this edge should belong to one, two, three. Okay. Assume that the hash of one, two, three is also the same. In that case, the edge will get stored over here. But there is also an inverse edge. There is an edge from post 345. To a destination of person 123 of type like by. This helps you find all the people who have liked the post. That's the reason why we have an inverse edge.

00:14:44	But how do you store this? The source is going to be three four, five. That is going to hit some other shard. So for every like you are having multiple shots that are being hit, basically two shards are being hit. Now this is not a huge problem in terms of write amplification, but the problem is that you have two shards which are being hit. It's possible that this shard accepts the right, but this one doesn't for some reason. Either there is a clock skew or you know the shard is down. This shard did not accept the right.

00:15:16	This would lead to data inconsistency. To avoid this, what Tau does is force the edge. The initial edge. Anyone else can be taking this initial edge. Let us say like the blue one is the first edge to persist to the shard first. So once this operation is complete, then the application server will say, now persist only after this response to one will you persist. Edge number two okay. So this is being managed by the client. Now the benefit of this is in case the first shard fails to take this edge.

00:15:58	Then you will retry. And you keep retrying. Only after this response the client tries operation number two, which is persist to this shard. But what happens if this shard fails? Okay, the first one went through, but the second one has failed. So you have an edge for person to post, which is the like edge, but you do not know. If you look at the post, you can tell all the people have liked it because this person is missing. That would lead to data inconsistency again, and this can't be avoided. Okay, you can have a transactional relation.

00:16:30	Also here you can make the client run a transaction where it is going to revert this edge. But that is not only slow, it's also complex in a distributed system especially. So what memcache client does is just leave this hanging edge and eventually a cron job. Goes and finds all hanging edges and completes them. Okay. So there will be a retry of this short three edge. Now there is a neat trick which tile uses. We had mentioned that the number of read requests on 99.8% of total requests, which means for every thousand requests,

00:17:09	you are going to get just two writes and 998 reads. The read operations that I have are very, very large as compared to writes. I should really have a lot of read replicas. If you have 1000 replicas in total in your system, then every write operation is going to result in 1000 replica shards being affected on every write operation. We are going to be taking all the shards in our system, and we are going to be propagating this update. Let us say this edge has been added to this shard. So you have a new edge.

00:17:52	This event is going to be propagated to all shards in Facebook. The reads on those replicas will be slow because you will have evicted entries and the writes will be extremely slow. The write amplification is going to be very, very large. So how do you avoid this? What you do is you instead split it into two parts. This entire architecture, this entire system is split into two parts. One is you have primary shards. And the second type is replica shards. They serve the same range. So 0 to 200 and 0 to 200.

00:18:32	Greater than 200. This is the right shard. And you have another read shot over here. Whenever there is a write operation, you're just going to go and hit the primary shards, okay? And the primary shards are going to be propagating these updates using some sort of a message queue. If you have seen the memcached reader, you know I'm not talking about a message queue. I'm actually talking about a change data capture solution. In the case of MySQL. We mentioned that this is a build log. Facebook has written a wrapper on top of this and called it mixed screen.

00:19:07	This actually propagates all of the events to only primary shards. So this shard will not get the event. But this other primary shard will get the event. Now on every right you do not have 1000 replicas being updated. You do not have a write amplification of 1000. You instead have a write amplification of, let's say ten. Okay, because only 2% of the nodes need to be right, nodes need to be primary nodes. The rest can all be read nodes. And in the case of Facebook they just have one primary shard for a key.

00:19:41	Okay, there are many replica shards for a key range, but there is only one primary. So this allows you to have read after write. Consistency. And the way this is done is very, very clever. We already talked about Max actually propagating updates, but the other thing is whenever a client actually connects to your system okay, you have a client here, you have a client, you then never directly connect to a primary shard. This edge that you see is actually never going to happen. All right, operations

00:20:17	are first sent to the nearest read replica. Okay, so let's take the red line and let us say that the first write operation goes to a read replica. The read replica then forwards it to the primary one. Primary has many read replicas, but a read replica knows where its primary is goes. There. This read replica will always forward to the primary here. The primary is going to actually persisted in the database. Take the response and give it back to the replica shard. So this shard is consistent. Okay, future reads on

00:20:51	this shard will have the right reflected. That ensures that you have read after write consistency. Now there's one thing that we should discuss in a social media system, it is very common to see viral events happening. You can scale your shard, you can scale the entire graph store. But viral events means you need to protect your shards also from too many requests. Okay. Things like rate limiting, back propagation, short circuiting is what we are looking at. Facebook does it in another interesting way.

00:21:22	What it says is if you have a lot of heat on a single shard, it's possible that everybody is suddenly looking for a post from shadow. Come. Okay, so this is the source ID. Now Shadow can is a very famous person. The post will also become very famous. That post is going to have thousands of edges. Maybe hundreds of thousands of edges, which are likes, comments, shards and so on. Okay, so this node is extremely hot if you store all of these edges, which in total maybe millions, millions of edges in a single shard, then that shard is going to get super hot.

00:22:06	It doesn't matter what you do in terms of reads and writes, this shard is just abnormally hot for a while. To mitigate this problem slightly. You can read out by source plus type. So all likes on this post are routed to one shard. All comments on this post that is a different type of edge are routed to another shard and all shards on that post. So source ID plus type or outer two on the shard. It mitigates the problem in some cases, but it doesn't in many other cases. The number of likes on a single shard cure may still be very high,

00:22:43	and in the worst case, there's going to be a comment by Amitabha chan congratulating shard of comment something. The post was super hot. Anyway, now you have a super hot comment also and maybe news media is now promoting this. So even if you shard by source plus type, it may not be enough. What do you do here? Just query the DB. Okay. You have a client which is asking for this post. The shard is too hot. If the shard is failing, start quitting the database. And you might think, well, why don't you add more read replicas here.

00:23:20	It won't help because if you add more read replicas, you have the same data that is being stored in these shards. The problem is that you can't store this post or this single comment. The problem is you can't store thousands or lakhs of likes in a single shard. Okay, the amount of data is too much. You can't have all of this data in a single shard. There's no point to try to push it into a single shard. You have a LRU policy. If you try to host all of this data in this shard, all you're going to have is thrashing.

00:23:52	You're going to get like for Gaurav. Okay, you fetch it from the database, somebody else comes at the same time from the US and they want to see a like from their frame. So you go and fetch that from the database instead of having thrashing, instead of having all of these evictions and very poor performance for the shard, just query the database directly. You are going to get it from there anyway, that's it for now. This is a very interesting paper to me. It's one of my favorite papers because I look at the system,

00:24:18	which is a graph store, and it feels like a graph store. It moves like a graph store, but internally it is memcached. It's using max scale. It's using all of the algorithms that memcached uses to build a graph, store. So instead of trying to build something from scratch, Facebook built its own graph store on existing technology. Thank you for watching this. If you liked the video, then I have a website called Interview Ready where I teach system design. So I have a full fledged system design course with over 300 videos.

00:24:48	You can check that out. Until next time, see you. Bye bye.

