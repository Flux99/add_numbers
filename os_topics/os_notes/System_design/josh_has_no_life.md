00:00:01	hey everyone and welcome back for contexts my girlfriend is sitting in the other room she's very real and is listening to me recording this which is freaking me out and giving me stage fright so uh I'm going to follow the age-old tip of just imagining your audience in your underwear that didn't work okay uh that in fact made things a lot worse I'm just going to try and not pee myself out of fear which is going to be hard cuz I drank a lot of chalky milk and appy juice at work today and uh we're going

00:00:26	to see where this goes all righty So today we're going to talk about tow which is a paper out of Facebook in 2013 uh for those of you who uh actually drink which is probably about 1% of you on this channel uh this is not in fact the club for underage drinkers uh I thought so too I was thoroughly disappointed and it turned out to just be a 13-page paper and it's quite a bit of a Christopher Nolan level mind at that so let's go ahead and get into it uh this is Facebook's distributed graph database uh it's going to be

00:00:54	responsible for storing their social graph uh so ideally it's going to be able to support a billion reads and a million writes per second uh they support this through a ton of caching and it's also sharded and Geor replicated because data centers can and do go down um in terms of our actual background here for setting up the problem basically what Facebook does is it's going to show a bunch of personalized content to all of our users so we have our social graph uh we have privacy settings we have probably some

00:01:20	random ml models that get computed on top of that and I guess I've been gaslighting everyone because all of these um systems design I guess uh home feed videos that I've been publishing like Twitter uh the kind of idea there is to do a lot of extra work on the right path and push a bunch of content to the user uh on the contrary here what we're actually going to be doing is doing all of our work on the read path so if we're going to be doing that and also accounting for the fact that there are disproportionate numbers of reads

00:01:46	compared to writes in something like a Newsfeed system or pretty much just any sort of Facebook page this system has to be incredibly fast for reads hence that kind of you know SLO of a billion reads per second cool so we want to be able to do that and what actually is the social graph because I guess that's a bit of an abstract concept well basically Facebook is going to take all of its data and model it in terms of a bunch of nodes and edges like any other graph that we might and so uh for example here oh boy

00:02:13	this one's about to embarrass me we've got LeBron James over here he's a person I obviously am friends with him because I'm very cool and then unrelated over here I have no idea how this one happened we've got kop at the bottom now I'm going to have to quiet down shortly uh but we may notice that I authored a post where it seems she's tagged there I have no clue when or how or why that happened uh but the idea is we've got a bunch of objects which are nodes so basically the object is representing

00:02:42	things like posts uh people comments locations uh potentially even tags but probably not tags tags more of an action um basically anything that uh represents an entity in Facebook and this ultimately is going to allow for a super flexible data model uh such that we can you know pretty much represent any sort of pattern and then query it through a variety of Facebook systems so um this is like a multi-tenant uh graph database and what that means is that you know even if you have one UI for a certain

00:03:10	amount of data uh through one Facebook application another one could also query the same stuff and display it differently um so besides the objects we also have associations so these are basically actions between two objects that happen at a point of time so for example if I have a post over here I may be able to tag someone in that post and similarly you can also have reverse associations so in the same way Kina can also be tagged at in a given post and so each one of these edges no I'm not talking about edging for discipline uh

00:03:39	that's a different thing I got excited too uh this is going to have a time stamp associated with it so all of these nodes are going to have uh a key value pair associated with them that's going to allow us to store any metadata that we need um each one of these edges also has a type associated with it uh which is just going to allow us to query them at finer granularity and build out uh database indexes around that cool so in terms of what Facebook was doing before they created too uh it was actually fairly similar on a surface

00:04:09	level but uh you know it was causing them a lot of problems so basically they were uh using my SQL to actually store all of the nodes and edges so they're not using a graph database they're using a non-native database here uh which is just going to be a relational one and then they have memcached over to the side and basically because this is a look aside cache what you're going to do in Step One is try and query from the cache if you have a cache Miss uh you go over to your database and then the

00:04:31	client is responsible for actually updating the cache in step three to make sure that we have all the information that we just read so that's good and in an event like this the cache is basically responsible for holding objects and also uh those kind of adjacency lists or perhaps subsets of edges that are originating from that object so there are a few problems with this they outline in the paper so number one uh mcash is like a very very basic key value cache so they don't really have good operations for dealing with

00:04:58	list data uh what that means is that you know if you want to insert an element in the middle of a list you can't really do so uh if you invalidate a list entry or invalidate a key uh value entry uh invalidates the entire list rather than just removing one Edge from it so that's a problem uh it's not too hard to fix but obviously they have more than that which is kind of the need for this whole thing number two is that you can have many uh many clients at the same time basically concurrently writing to the

00:05:23	same cash so if I have another client over here that's doing the same exact process with the look aside cache you know reading from my SQL and writing to it uh we could be writing to or reading from my SQL in different orders and then writing to the cache in different orders and there's not really any good logic to actually serialize or order these rights which is going to be a problem and then also just in terms of getting read after write consistency that's a little bit tougher as well because you know if I'm

00:05:48	this guy over here let me erase this Edge I can write to a database I can then read from the cache and my right's not there and that's a problem so if I'm the client now I basically have to keep track of all the rights that I've made so that I know I'm about to not get read after write consistency when I read from the cache and make sure to actually fill it up so that's going to be a problem as well it's not really a huge deal uh but it's just extra client side logic and uh it's frustrating to maintain cool so let's actually talk

00:06:17	about um how objects and associations are themselves stored on disk basically within the MySQL tables uh Facebook's going to use two different ones they have one objects table and they have one associations table now this is going to be a really huge data set so of course we're going to have to partition and chard it uh but the problem is there's no really natural way to do all this partitioning the social graph itself is going to be tightly coupled uh you know everyone is friends with one another

00:06:40	there are tons of mutual friends there are tons of post Connecting People same with comments and same with likes so there's not really a good way that we can kind of you know take clusters of the graph uh like this and break it up such that you know only two nodes are on one Shard and two nodes are on another Shard and that way we can kind of isolate all of our is to be on one node so the one thing that they do do uh to kind of get a little bit better data locality here is let's say I have one node over here and it's

00:07:09	got all of these edges originating out from it these guys will all be on the same uh database chart that being said if they're reverse edges going to it those will not and then I guess the the final piece of this is going to be the aspect of creation time locality so maybe this guy was created at T equals 1 this guy's created a tal 2 and this guy's created at tals 3 generally speaking when we're loading uh you know comments on a post or posts in general uh or you know maybe friends we want to do so in a way that

00:07:41	it's probably in Reverse chronological order uh so you know most recent to least recent and especially when there are a lot of these edges um you know coming out of one object you want to be able to do so in a way that uh you know supports pagination as well so ideally being able to actually pre-order all of these things by their timestamp uh from most to least recent is also going to help us speed up our queries a little bit it's going to allow us to run more efficient range queries when we're

00:08:05	actually looking for you know say all the most recent comments on a popular post or something like that cool so the first thing that I'm going to cover in terms of what TOA actually does to try and solve their goals here is to make something called a cash tier so a cash tier is basically going to um be representing the entire data set uh but it is responsible for covering all reads and writes that go to the underlying MySQL shards so basically in terms of what we do on MySQL we're splitting everything into database

00:08:39	shards right we've got this guy here we're doing that so we can load balance and also just you know because we have a massive data set and then uh so that's actually going to split out the data on the underlying databases but then how can we protect those databases from you know serving a ton of reads and going down and getting things like uh thundering herds well we need caching so we have all these different shards for the database and then we have multiple different cache servers Each of which

00:09:03	are going to be responsible for in total uh holding all of the shards and those cache servers going to comprise something known as a cash tier the tow paper just makes up a bunch of annoying ass terms in this paper and so I have to use them but I would prefer that I didn't but we're going to anyway so we have a cach tier as you can see I've got three nodes over here in the cach tier and collectively that is going to be covering the two nodes that are in our mySQL database so basically we're using consistent hashing

00:09:34	in order to uh assign shards to caches within a tier and note that you know because of the fact that this thing is so tightly coupled if I'm making a read to uh one of my caches it may have to actually go and then subsequently make reads to the other caches in the tier and so we're going to have quadratic scaling here right so if I go from three caches here to four caches here the number of connections that we have uh you know goes from six to quite a bit more than that I guess uh you know 4+ 3

00:10:02	plus 2 + 1 so 6 to 10 that's not good we don't want that and uh you know we don't want quadratic scaling so ideally we aren't just making these massive cach tiers with a bunch of different elements because that is going to make things very inefficient for us and there's going to be a ton of network IO that we incur so we don't want this being too large uh additionally a nice thing about having this cach tier is that we're actually doing right through caching so if I'm making a right over here you

00:10:29	might know note that this is going right to the database uh so not only do we get uh you know read your rights consistency because the second I make a write it's already in the cache um we're also getting basically protection from thundering herds and also serialization or ordering of Rights within the cache that is representing A Shard on the database so if I want to write to Shard three maybe I have another client who's doing the same thing this guy might be receiving request on multiple threads

00:10:55	but at the end of the day they're probably going into some sort of blocking queue that's going to order them and then that way we're kind of ensuring that every single right is versioned and we don't have to deal with concurrency there okay this is where things start getting really annoying and I'm probably going to lose my mind this is when we add more cash tiers so I guess it's not enough to Simply have one cash tier because like we know we're going to just keep getting more and more scale here

00:11:18	and adding more nodes to that cash tier is not really feasible the reason being that again we have a quadratic number of connections and so what Facebook chooses to do is they introduce this concept of one single leader cach tier and then many follower cach tiers so I'm going to try and represent through these arrows uh kind of data flow that we actually see here so uh let's imagine I'm making a write okay uh I'm going to start with this writer over here so if I'm making a right from this client and that right is

00:11:47	to Shard number one that's going to only start at a follower cach tier all clients are only going to be connected to followers from there this client is then going to forward that right over to the leader cache tier uh for for that Shard and then from there that's going to go to the database cuz ultimately the database is going to be the source of Truth two things are going to happen so first off this right is going to go right back to the originating follower cache that uh started it because we want

00:12:14	to make sure that's a synchronous process so that we get read your own rights consistency and then for any of the other follower cache tiers so for example we've got one tier over here and we've got one tier over here for this cash tier over here we're going to have an asynchronous invalidation so there's no guarantee of when exactly that invalidation is going to come but it is going to come eventually and so all of a sudden we now have this eventual consistency semantics where you've got a guy over here he's reading from this

00:12:42	cache and then uh there's no guarantee of when the data on the cach is going to be invalidated so he may be reading stale data for a little bit uh why do we do invalidations as opposed to actually sending the data itself to that cache generally invalidations are considered to be more efficient because the data could be pretty large and there's no guarantee that this client is actually going to read it so we'd rather just lazily load it rather than eagerly load it um in terms of other benefits for

00:13:08	writes and reads if you're reading from a follower cach tier that read is eventually going to be forwarded over to the leader cache representing that Shard which again if there's still a cache Miss over there that is going to go to the database it's worth noting that for reads ideally what we want is for this cash to be able to answer the query or if not for this guy to be able to but in the worst case scenario we're going to always always have to fall back to the database which is unfortunate but

00:13:31	there's not really much you can do about that uh another nice thing is again we continue to have all of our rights being serialized or ordered in the tiers of caches so this guy is going to order the rights between these two clients and then at this cache over here we're ordering those rights with a combination of this right over here so this is really nice right we're blocking a lot of the rights and read accesses to the database uh we can perform throttling if we have to uh everything is ordered and

00:13:58	we get eventual consistency while also getting read your own right consistency now a nice thing is that as long as we're quarrying the same follower tier over here right this guy's reading at some point as long as we keep doing that we should also get monotonic reads so we shouldn't never see State go backwards in time now the truth is uh I put a little asteris here saying that's almost always there's a raise condition there that we can go through uh later in the video uh which is pretty small and

00:14:21	unlikely but it's worth noting that it can happen nonetheless okay so as if this wasn't a problem enough and already didn't piss me off of course course they had to make this thing geo-replicated because at the end of the day uh you could have what we're having in LA right now where you have wildfires and it could literally just take out an entire region of databases so we need to be able to have support for everything in multiple geographic regions so they Define a new term called region because

00:14:47	I guess a data center we all know is a you know a building of servers in it but they don't want to have a full copy uh or a full replica of the database in every single data center because they have tons of data centers so instead what they choose to do is Define uh you know groupings of data centers called regions so regions are data centers where the network latency between them is less than a millisecond and what they're going to say is okay within a region we need to have a full copy of the tow data set now this is going to be

00:15:13	asynchronously replicated just using normal MySQL replication again it's going to be sharded as well so you know you have some primary databases like these guys over here and then you have your backup MySQL databases over here so um I guess the idea here again is you still have eventual consistency right we're still getting that um but we're able to now get nice read latency even within a backup region so I'll explain how that works right here so if I'm a client I'm going to connect to my closest uh follower cash tier so again

00:15:43	this we've got now follower cash tiers follower regions but I'm talking about now this guy this follower cach in the backup region that follower cach if we have a cash Miss is now going to forward its requests to the leader cache in its region because the leader uh or the backup region is going to look identical to the primary one and then if all fails there that read is still going to be serviced locally uh by the local replica so again our reads are still going to be fairly low latency they're going to be

00:16:11	eventually consistent but that's okay uh because you know we're willing to take that trade-off in order for good speed we don't really care about evental cons or you know strong consistency here because it's a social network so not really a huge deal rights on the other hand do eventually still have to go to our single leader it's not like this is using a multier database setup so if we have a writer over here this client he's going to write first to his local follower cache that local follower cach

00:16:37	is going to forward it to the local leader cache the local leader cach is going to forward it to the primary leader cache the primary leader cach is going to forward it to the actual database and then from there again this has to be a completely synchronous process because we still need to ensure that we're getting read your own right consistency because that's fairly important for a social network if you post you want to see that you made a post right after so again that whole thing is then synchronously replicated all the way

00:17:02	back through to this cache and then uh we're going to be doing asynchronous replication for all of the other caches that are responsible for the same shard in different tiers so right off the bat you know maybe this cache over here is also responsible for it or this cach these guys are going to be receiving asynchronous invalidations and then over here these guys are also going to be maybe uh it's this cach that's responsible for it this guy is also going to be asynchronously invalidated however the

00:17:32	one kind of caveat here is that that's not actually going to be coming from this leader cache in the primary region but rather the um invalidations are going to be embedded in the actual database replication flow the reason for that is that if it's possible that this guy were to get invalidated uh before the database itself actually applies the new right then you know maybe I have another reader here it comes here it says hey give me new data this guy is already invalidated it says I don't have any

00:18:02	data return you let me go to the database and then the database doesn't yet have the new right so then you would uh end up just getting an old right back so it's important basically that you know the local uh replica of the database applies the cache and validation uh before the asynchronous cach and validations come through cool uh actually one more thing that I will note before I move on is so even though we've said so far mostly that we're doing cash invalidation uh that would be kind of problematic for

00:18:31	when we're caching a list of edges coming out of one particular node because then we would delete that entire list of edges so I believe to by default they'll uh you know return up to 6,000 edges at a time from a source node and so if you were to you know change or delete one of them then you'd have to delete all 6,000 you'd be doing a ton of work so instead what they'll do is something called refills where basically you know you're just modifying the list instead and you know modifying or removing any Edge that has to be removed

00:18:58	while keeping the rest of them around okay so a few actual notes on the caches themselves because right now I've been talking about them at a pretty high level basically uh they're all write through caches which is nice because again you're going to get read after write consistency additionally uh as opposed to using something like TCP or some sort of other in order Network protocol um one thing that can happen is if this cache for example this leader cache right here is making many different requests to this database and

00:19:26	using the same port to do so uh you know we can use port multiplexing to get concurrency out of that however if one request to the database is particularly slow that is going to stop all of the others from returning so basically what they do in taao is they decide to actually use an unordered communication protocol probably something like UDP uh and that way they're going to be able to fit quite a few more requests in another thing to note is that uh while I didn't discuss this too much so far there's

00:19:51	also this concept of having an inverse Edge right so everyone who's friends with one another in Facebook you know that is a symmetric thing so I'm friends with you you're also friends with me so if I unfriend you that means you're also unfriending me and so there are certain operations that need to be uh you know executed both on an edge and also its inverse Edge however if you recall because of our sharding patterns here uh the edges and inverse edges are themselves likely going to be in different shards and so I guess it

00:20:17	becomes a question of whether we want to try and update these atomically via two-phase commit or just try and keep our read and write through puts relatively fast and avoid trying to do that for consistency sake so in too ultimately what they're going to do is not execute those automically they'll basically you know do those inverse operations uh on a best efforts basis that being said they also just have some background Chron jobs or processes that'll read through all the data look for any sort of hanging Association

00:20:43	where you know one Edge is updated but it's inverse is not and evidently they're able to tell that through things like version numbers and then they can go ahead and fix them in the background so again you're still getting this kind of eventual consistency where things may not look good for a few seconds or a few minutes even but eventually they'll be okay cool uh finally like I mentioned the Cache can basically hold three things it can hold objects so that's going to be the nodes of our graph it

00:21:06	can hold subsets of Association lists so subsets where they're probably length 6,000 at a time or Association counts so the nice thing about that is uh these caches are actually graph aware so for example if I have an association count you know I'm saying okay from Jordan how many edges of type friend do I have uh you know that were created from yesterday to today and if the cash value for that or the number of them is going to be zero and then I make a separate request to the cash saying hey get me you know

00:21:40	the last friend that Jordan has between yesterday and today so I'm no longer looking for you know a count of associations but rather one specific one uh I am C graph aware such that I can see there's zero of them and then I can just instantly return an empty set or a null value or something like that uh unsurprisingly the cash is going to be using lru or least recent use replacement just to ensure that uh we can evict entries when they either you know uh are not useful or we're just running out of

00:22:07	space cool for cach details so we've got a few different terms that they made up here which yet again always annoys me but uh what do I know because these guys are all smarter than me uh we've got Arenas which are just going to be partitions of the actual memory in the cache itself and then within every single type we've got objects Association lists and Association counts so basically we will probably be using a different arena for objects versus Association lists versus the association counts and that's just going to ensure

00:22:35	that we can kind of scale them independently and allocate different amounts of memory on each node uh towards caching those that being said you know not all objects are equal not all Association lists are equal and so they introduce another concept called slabs so that's going to allow us to do further breaking down so for example each Edge itself has a type right there's just one string associated with it so maybe we really want to aggressively cach friend relationships don't care about you know tagging

00:23:00	relationships then the slab for friends could be bigger and within each slab we're still using lru so uh that is going to allow us to ensure that we can evict things when we run out of memory so one thing that they also touch upon that I found pretty interesting and hadn't heard of before was this concept of a direct mapped cache so keep in mind that in addition to caching objects and Association lists they're also caching Association counts for a given Source node and uh Association type so the

00:23:27	interesting here uh in thing here is that because this is just a number that's probably going to be you know four or eight bytes probably eight bytes uh if it's a long and so the actual overhead of holding pointers by doing something like a normal lru cache is going to be pretty large right because normally we would do that with like a doubly link list and a hashmap and so these two pointers themselves are basically going to be keeping probably 2third of our space in the cache and so obviously Facebook wants to avoid that

00:23:54	so what they do instead specifically for the association counts is use this direct map cache so think of this just as like an array where each slot of the array can itself hold eight numbers so you know you'll use some sort of hashing function to take an association count value and plop it into the array like so and so that's going to deterministically tell us uh which entry of the array we would have to look in and then which uh within each one of these buckets that holds eight things you know we'll actually shift the

00:24:22	elements around physically within their own ordering so that we can perform lru so that's going to use a little bit more CPU because you know now you have operations to do where you're like shifting elements around in that bucket uh but at the end of the day they find it to be more efficient in terms of the space being used and evidently the small you know CPU penalty is not going to be a big deal here cool let's go ahead and talk about hotpots um you know obviously as a social network or a social media site uh

00:24:48	certain people are famous uh I wish I could say the same for myself but unfortunately I am not so you know if Cristiano Ronaldo or some other hottie like that decides to post he's going to get a lot of interactions so basically despite us using consistent hashing consistent hashing is going to ensure that the actual amount of data on each node is hopefully relatively frequent unless you just have some guy like Elon on Twitter who just posts constantly and does so orders of magnitudes more than everyone else but the point is certain

00:25:16	things are just going to be read and written to quite a bit more frequently so what we can do is something known as Shard cloning so even though the caches are themselves all mapped to a subset of the shards there's no reason that each cach cash especially within follow tiers has to have an exclusive uh you know lease or mapping to a particular cache right it's very possible that multiple caches when the follower tier can be serving the data for one Shard if they do uh it's worth noting that they also

00:25:42	have to be signed up to receive invalidations from the leader tier uh which is fine it's not that hard it's just you know more messages for the leader tier to actually send out we can also introduce client side caching so if I'm a user device and I'm reaching out to TOA or maybe I'm an application server which is more likely basically what we're going to do is all of these caches in our follower tiers uh can actually keep track of how often particular data has been hit or accessed and if it's been accessed often enough

00:26:10	then all they have to do is you know return the data to the client and also say hey cach this thing locally here's a version number associated with it and if you reach back out to me again uh provide me with the version number and if you already have that data I won't send it back to you so this is just going to limit some of the actual uh Network IO that's being done uh from the actual cache back to the application server now it is true that you know you could always just have the client keep that data with a TTL or something like

00:26:34	that and then not even reach out to any caches in the first place uh but you know then you may not get any invalidations or we're not sending the client invalidations in the first place because we don't want uh you know the number of network messages that we have to balloon up too much so this is Facebook's solution of dealing with that okay quickly we're going to talk about the consistency model because thus far you know we've touched upon it a bit but let's try and Hammer it down I mentioned there's a race condition here

00:27:00	which is a little bit problematic so we'll go over that so basically TOA is going to uh you know do eventual consistency like we mentioned because strong consistency dealing with consensus anything along those lines is just going to be too expensive here now that being said you know you still do have your database as a source of truth but a lot of the time clients are going to be reading from caches uh and you know reading from stale database replicas and so that's where the eventual consistency comes from they do

00:27:25	have read your own rights consistency which is nice and they get by virtue of doing all of this right through caching and then finally ideally you should be getting monotonic reads within a tier right you should never see the data that you're reading go back in time that being said this is not always the case and here's one Edge case where it actually could uh break down so let's say again we're doing this kind of primary region backup region thing because again to is Geor replicated and we have a client over here the first

00:27:52	thing the client is actually going to do is make a right to its local follower cache that it's connected to that is going to forward it to the local leader cache and step three that gets forwarded to the primary leader cache step four that gets forwarded to the primary region database now in theory the primary region database is going to replicate this change to the backup region database and that's how you know this guy would know to invalidate any old data or something like that that being said this process is

00:28:23	asynchronous so there's no guarantee on when it actually happens it could never happen if we have a network partition or something like that so in step five maybe this doesn't happen for a super long time for some reason because I decided to cut some cable between the two of them this value that was written and is cach due to our right through semantics actually gets evicted you know maybe we needed to make space for other data in the cache in Step six this same client now is actually going to read the

00:28:48	data so he actually wrote the data he reads it again and then it's not there and then that's the one race condition that we do have that being said this shouldn't really happen often in practice because of the fact that for this to happen this eviction step needs to happen before the asynchronous database replication and that doesn't happen too often in practice because you know this is based on lru which implies that this cache has to be getting used a whole lot for that data to be evicted okay the final thing that we'll

00:29:16	talk about uh before I wrap this up and uh you know look like a nerd and can't go back in her eyes is Fault tolerance so uh basically what we have here is if a client cash goes down uh it's just going to go ahead and reach out to another one so uh what that means is that we're going to actually be switching tiers right because maybe we have one tier over here and a leader tier so this is a follower tier follower tier leader tier within a local region the client if it tries over here and that doesn't work it's coming to this

00:29:47	tier and now all of a sudden you know your reads may go back in time because the cash just may not be as up to date if the master database fails so this would be the master B database in the primary region we're going to try and fail over to uh you know the same database in a backup region uh which is pretty typical if the backup regions database goes down basically let's imagine this is now not the primary region but the backup region and our primary database is actually over here recall that cach inv validations

00:30:23	are going over the replication stream so that no longer works so basically now the cash inval validations are going to have to go over some different process directly to the caches in the backup region but basically if the backup region goes down uh all we're going to do is forward all of our rights um to the actual primary cache for the leader over here and actually I believe we're going to do the same thing with reeds as well look who made a typo on their slide I will add it to the description because

00:30:50	I'm too lazy to edit uh number four basically is that if the leader tier is going to go down uh in any region basically we're going to temporarily replace it with a different member of its tier so if a leader tier server uh representing you know Shard one goes down the leader tier member that is representing Shard two and three will temporarily take on one uh and serve all of the rights corresponding to it okay in conclusion because my voice is starting to hurt the social graph is very large tightly coupled customizable

00:31:23	and this is going to be problematic because it basically doesn't allow you to do a push-based approach you have to resolve every single thing that you're showing to a user at read time and just generally read dominated right like the majority of us are probably lurking I haven't posted on Reddit in a really long time I'm just reading on it and being disappointed in everyone basically the idea is we're doing a ton of reads uh we have to resolve very complicated logic all of the logic that we're resolving is spread over many nodes so

00:31:47	we have very tled uh or tightly coupled uh queries that we're making and then the data is super large so it still needs to be sharded so basically all of these caches within a tier are communicating with one another constantly that has to be as fast as possible and the way that to resolves this is by basically brute forcing it as much as can be we scale this thing out by using multiple layers of caches uh we ensure that everything is communicating with one another using out of order message protocols it's all stored in memory as

00:32:13	much as possible I think they're getting like 96% cash hit rates and then finally we're using a relaxed consistency model and that's just going to ensure that we have minimal coordination overhead while still you know ensuring things like read after write consistency eventual consistency and also so monotonic reads within a tier well guys hope you enjoyed this video uh I'm fairly embarrassed of what I've just done but that's okay and I will see you in the next one

